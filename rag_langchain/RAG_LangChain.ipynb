{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f82a8b9",
   "metadata": {},
   "source": [
    "# **PDF RAG**\n",
    "\n",
    "Authored by [Kalyan KS](https://www.linkedin.com/in/kalyanksnlp/). To stay updated with LLM, RAG and Agent updates, you can follow me on [Twitter](https://x.com/kalyan_kpl).\n",
    "\n",
    "- Step-1 : Extract the PDF text\n",
    "- Step-2 : Chunk the extracted PDF text\n",
    "- Step-3 : Create a vector store with the PDF chunks\n",
    "- Step-4 : Create a retriever which returns the relevant chunks\n",
    "- Step-5 : Build context from the relevant chunk texts\n",
    "- Step-6 : Build the RAG chain using rag prompt, LLM and string output parser.\n",
    "- Step-7 : Run the RAG chain to get the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdae6d9d",
   "metadata": {},
   "source": [
    "## **Install and import libraries**\n",
    "\n",
    "- PyPDFLoader uses `pypdf` python library to extract text from PDF document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c387cc33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/opt_einsum-3.4.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/dill-0.3.9-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/nvfuser-0.2.13a0+0d33366-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/texttable-1.7.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_utilities-0.11.8-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/looseversion-1.3.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/igraph-0.11.8-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/opt_einsum-3.4.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/dill-0.3.9-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/nvfuser-0.2.13a0+0d33366-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/texttable-1.7.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_utilities-0.11.8-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/looseversion-1.3.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/igraph-0.11.8-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -qU langchain langchain-community langchain-text-splitters\n",
    "!pip install -qU langchain-openai langchain-chroma pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0510835c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters  import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13ab5b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain.schema import Document\n",
    "\n",
    "def pdf_extract(pdf_path: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file using PyPDFLoader.\n",
    "\n",
    "    Parameters:\n",
    "    pdf_path (str): The file path of the PDF to be extracted.\n",
    "\n",
    "    Returns:\n",
    "    List[Document]: A list of Document objects containing the extracted text from the PDF.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"PDF file text is extracted...\")\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    pdf_text = loader.load()\n",
    "\n",
    "    return pdf_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f6faaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the PDF file\n",
    "import requests\n",
    "\n",
    "pdf_url = 'https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf'\n",
    "response = requests.get(pdf_url)\n",
    "\n",
    "pdf_path = 'attention_is_all_you_need.pdf'\n",
    "with open(pdf_path, 'wb') as file:\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f05e259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF file text is extracted...\n"
     ]
    }
   ],
   "source": [
    "pdf_text = pdf_extract(pdf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba9caa4",
   "metadata": {},
   "source": [
    "## **Chunk PDF text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21272659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_chunk(pdf_text: List[Document]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Splits extracted PDF text into smaller chunks using RecursiveCharacterTextSplitter.\n",
    "\n",
    "    Parameters:\n",
    "    pdf_text (List[Document]): A list of Document objects containing extracted text from a PDF.\n",
    "\n",
    "    Returns:\n",
    "    List[Document]: A list of chunked Document objects.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"PDF file text is chunked....\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "    chunks = text_splitter.split_documents(pdf_text)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f91b5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = pdf_chunk(pdf_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7846b7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks = 40\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of chunks = {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab22a079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar∗\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@google.com\n",
      "Aidan N. Gomez∗†\n",
      "University of Toronto\n",
      "aidan@cs.toronto.edu\n",
      "Łukasz Kaiser ∗\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhin∗‡\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to\n",
      "be superior in quality while being more parallelizable and requiring signiﬁcantly' metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'attention_is_all_you_need.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed7605e",
   "metadata": {},
   "source": [
    "## **Create Vector Store**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c0bc677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_store(chunks: List[Document], db_path: str, model_name: str) -> Chroma:\n",
    "    \"\"\"\n",
    "    Creates a Chroma vector store from chunked documents.\n",
    "\n",
    "    Parameters:\n",
    "    chunks (List[Document]): A list of chunked Document objects.\n",
    "    db_path (str): The directory path to persist the vector store.\n",
    "\n",
    "    Returns:\n",
    "    Chroma: A Chroma vector store containing the embedded documents.\n",
    "    \"\"\"\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model_kwargs = {\n",
    "        \"device\": device,\n",
    "        \"trust_remote_code\": True\n",
    "    }\n",
    "    embedding_model = SentenceTransformerEmbeddings(\n",
    "                        model_name=model_name, \n",
    "                        model_kwargs=model_kwargs\n",
    "                    )\n",
    "\n",
    "    print(\"Chrome vector store is created...\\n\")\n",
    "    db = Chroma.from_documents(documents=chunks, embedding=embedding_model, persist_directory=db_path)\n",
    "\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c15d651d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Alibaba-NLP/gte-multilingual-base were not used when initializing NewModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chrome vector store is created...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "db = create_vector_store(\n",
    "        chunks=chunks, \n",
    "        db_path=\"./chroma_alibaba_gte.db\", \n",
    "        model_name=\"Alibaba-NLP/gte-multilingual-base\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c2e64b",
   "metadata": {},
   "source": [
    "## **Retrieve relevant chunks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "06663fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context(db: Chroma, query: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Retrieves relevant document chunks from the Chroma vector store based on a query.\n",
    "\n",
    "    Parameters:\n",
    "    db (Chroma): The Chroma vector store containing embedded documents.\n",
    "    query (str): The query string to search for relevant document chunks.\n",
    "\n",
    "    Returns:\n",
    "    List[Document]: A list of retrieved relevant document chunks.\n",
    "    \"\"\"\n",
    "\n",
    "    retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})\n",
    "    print(\"Relevant chunks are retrieved...\\n\")\n",
    "    relevant_chunks = retriever.invoke(query)\n",
    "\n",
    "    return relevant_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b8c52b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant chunks are retrieved...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the attention mechanism?\"\n",
    "\n",
    "relevant_chunks = retrieve_context(db, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2bae2f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of relevant chunks = 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of relevant chunks = {len(relevant_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "14ac136b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference 0\n",
      "'page_content='Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar∗\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@google.com\n",
      "Aidan N. Gomez∗†\n",
      "University of Toronto\n",
      "aidan@cs.toronto.edu\n",
      "Łukasz Kaiser ∗\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhin∗‡\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to\n",
      "be superior in quality while being more parallelizable and requiring signiﬁcantly' metadata={'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'book': 'Advances in Neural Information Processing Systems 30', 'created': '2017', 'creationdate': '', 'creator': 'PyPDF', 'date': '2017', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'eventtype': 'Poster', 'firstpage': '5998', 'language': 'en-US', 'lastpage': '6008', 'moddate': '2018-02-12T21:22:10-08:00', 'page': 0, 'page_label': '1', 'producer': 'PyPDF2', 'published': '2017', 'publisher': 'Curran Associates, Inc.', 'source': 'attention_is_all_you_need.pdf', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'title': 'Attention is All you Need', 'total_pages': 11, 'type': 'Conference Proceedings'}'\n",
      "\n",
      "Reference 1\n",
      "'page_content='the number of operations required to relate signals from two arbitrary input or output positions grows\n",
      "in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\n",
      "it more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is\n",
      "reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\n",
      "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\n",
      "described in section 3.2.\n",
      "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\n",
      "of a single sequence in order to compute a representation of the sequence. Self-attention has been\n",
      "used successfully in a variety of tasks including reading comprehension, abstractive summarization,\n",
      "textual entailment and learning task-independent sentence representations [4, 22, 23, 19].' metadata={'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'book': 'Advances in Neural Information Processing Systems 30', 'created': '2017', 'creationdate': '', 'creator': 'PyPDF', 'date': '2017', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'eventtype': 'Poster', 'firstpage': '5998', 'language': 'en-US', 'lastpage': '6008', 'moddate': '2018-02-12T21:22:10-08:00', 'page': 1, 'page_label': '2', 'producer': 'PyPDF2', 'published': '2017', 'publisher': 'Curran Associates, Inc.', 'source': 'attention_is_all_you_need.pdf', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'title': 'Attention is All you Need', 'total_pages': 11, 'type': 'Conference Proceedings'}'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(relevant_chunks):\n",
    "    print(f\"Reference {i}\\n'{doc}'\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93a12b8",
   "metadata": {},
   "source": [
    "## **Build context**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2535ca69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_context(relevant_chunks: List[Document]) -> str:\n",
    "    \"\"\"\n",
    "    Builds a context string from retrieved relevant document chunks.\n",
    "\n",
    "    Parameters:\n",
    "    relevant_chunks (List[Document]): A list of retrieved relevant document chunks.\n",
    "\n",
    "    Returns:\n",
    "    str: A concatenated string containing the content of the relevant chunks.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Context is built from relevant chunks\")\n",
    "    context = \"\\n\\n\".join([chunk.page_content for chunk in relevant_chunks])\n",
    "\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7e68229b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context is built from relevant chunks\n"
     ]
    }
   ],
   "source": [
    "context = build_context(relevant_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7e9c44a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar∗\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@google.com\n",
      "Aidan N. Gomez∗†\n",
      "University of Toronto\n",
      "aidan@cs.toronto.edu\n",
      "Łukasz Kaiser ∗\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhin∗‡\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to\n",
      "be superior in quality while being more parallelizable and requiring signiﬁcantly\n",
      "\n",
      "the number of operations required to relate signals from two arbitrary input or output positions grows\n",
      "in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\n",
      "it more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is\n",
      "reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\n",
      "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\n",
      "described in section 3.2.\n",
      "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\n",
      "of a single sequence in order to compute a representation of the sequence. Self-attention has been\n",
      "used successfully in a variety of tasks including reading comprehension, abstractive summarization,\n",
      "textual entailment and learning task-independent sentence representations [4, 22, 23, 19].\n"
     ]
    }
   ],
   "source": [
    "print(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863f522a",
   "metadata": {},
   "source": [
    "## **Combine all the steps into one function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "977679a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict\n",
    "\n",
    "def get_context(inputs: Dict[str, str]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Creates or loads a vector store for a given PDF file and extracts relevant chunks based on a query.\n",
    "\n",
    "    Args:\n",
    "        inputs (Dict[str, str]): A dictionary containing the following keys:\n",
    "            - 'pdf_path' (str): Path to the PDF file.\n",
    "            - 'query' (str): The user query.\n",
    "            - 'db_path' (str): Path to the vector database.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, str]: A dictionary containing:\n",
    "            - 'context' (str): Extracted relevant context.\n",
    "            - 'query' (str): The user query.\n",
    "    \"\"\"\n",
    "    pdf_path, query, db_path, model_name  = inputs['pdf_path'], inputs['query'], inputs['db_path'], inputs['model_name']\n",
    "\n",
    "    # Create new vector store if it does not exist\n",
    "    if not os.path.exists(db_path):\n",
    "        print(\"Creating a new vector store...\\n\")\n",
    "        pdf_text = pdf_extract(pdf_path)\n",
    "        chunks = pdf_chunk(pdf_text)\n",
    "        db = create_vector_store(chunks, db_path)\n",
    "\n",
    "    # Load the existing vector store\n",
    "    else:\n",
    "        print(\"Loading the existing vector store\\n\")\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        model_kwargs = {\n",
    "            \"device\": device,\n",
    "            \"trust_remote_code\": True\n",
    "        }\n",
    "        embedding_model = SentenceTransformerEmbeddings(\n",
    "                            model_name=model_name, \n",
    "                            model_kwargs=model_kwargs\n",
    "                        )\n",
    "        db = Chroma(persist_directory=db_path, embedding_function=embedding_model)\n",
    "\n",
    "    relevant_chunks = retrieve_context(db, query)\n",
    "    context = build_context(relevant_chunks)\n",
    "\n",
    "    return {'context': context, 'query': query}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751e2c89",
   "metadata": {},
   "source": [
    "## **Build RAG chain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e15fa667",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\",)\n",
    "model = model.to(device)\n",
    "\n",
    "# Build pipeline\n",
    "hf_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0,  # ensure CUDA\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e25e0928",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "class AssistantOnlyOutputParser(BaseOutputParser):\n",
    "    def parse(self, text: str) -> str:\n",
    "        # Extract everything after \"Assistant:\"\n",
    "        if \"Assistant:\" in text:\n",
    "            return text.split(\"Assistant:\")[-1].strip()\n",
    "        return text.strip()  # fallback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9d5a70ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\" You are an AI model trained for question answering. You should answer the\n",
    "  given question based on the given context only.\n",
    "  Question : {query}\n",
    "  \\n\n",
    "  Context : {context}\n",
    "  \\n\n",
    "  If the answer is not present in the given context, respond as: The answer to this question is not available\n",
    "  in the provided content.\n",
    "  \"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=hf_pipe)\n",
    "\n",
    "# Replace StrOutputParser with your new parser\n",
    "str_parser = AssistantOnlyOutputParser()\n",
    "\n",
    "rag_chain = (\n",
    "    RunnableLambda(get_context)\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | str_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1296fe69",
   "metadata": {},
   "source": [
    "## **Run RAG chain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c433bb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Embedding Model\n",
    "model_name=\"Alibaba-NLP/gte-multilingual-base\"\n",
    "\n",
    "# Set the chroma DB path\n",
    "db_path=\"./chroma_alibaba_gte.db\"\n",
    "\n",
    "# RAG query\n",
    "query = \"What is self-attention?\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "696706ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the existing vector store\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Alibaba-NLP/gte-multilingual-base were not used when initializing NewModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant chunks are retrieved...\n",
      "\n",
      "Context is built from relevant chunks\n"
     ]
    }
   ],
   "source": [
    "answer = rag_chain.invoke(\n",
    "        {\n",
    "            'pdf_path':pdf_path, \n",
    "            'query':query, \n",
    "            'db_path':db_path,\n",
    "            'model_name':model_name\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9f0a56e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:What is self-attention?\n",
      "\n",
      "Generated answer:Self-attention is an attention mechanism that relates different positions within a single sequence to compute a representation of the sequence. It has been successfully used in various tasks such as reading comprehension, abstractive summarization, textual entailment, and learning task-independent sentence representations. Unlike traditional methods like Convolutional Neural Networks (ConvNets) and Recurrent Neural Networks (RNNs), which rely on position-specific weights, self-attention computes weighted sums across all positions in the sequence, providing a more flexible and powerful way to capture relationships between elements in the sequence.\n",
      "\n",
      "In the context of transformer models, self-attention helps reduce the dependency between distant positions in the sequence, making it possible to learn longer-range dependencies efficiently. However, this comes at the cost of lower effective resolution due to averaging over multiple positions. To address this issue, multi-head attention mechanisms are employed, allowing for multiple independent attention heads to process the sequence information simultaneously, thereby improving both accuracy and efficiency.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Query:{query}\\n\")\n",
    "print(f\"Generated answer:{answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b990650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Self-attention is an attention mechanism that relates different positions within a single sequence to compute a representation of the sequence. It has been successfully used in various tasks such as reading comprehension, abstractive summarization, textual entailment, and learning task-independent sentence representations. Unlike traditional methods like Convolutional Neural Networks (ConvNets) and Recurrent Neural Networks (RNNs), which rely on position-specific weights, self-attention computes weighted sums across all positions in the sequence, providing a more flexible and powerful way to capture relationships between elements in the sequence.\\n\\nIn the context of transformer models, self-attention helps reduce the dependency between distant positions in the sequence, making it possible to learn longer-range dependencies efficiently. However, this comes at the cost of lower effective resolution due to averaging over multiple positions. To address this issue, multi-head attention mechanisms are employed, allowing for multiple independent attention heads to process the sequence information simultaneously, thereby improving both accuracy and efficiency.'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8333f11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
