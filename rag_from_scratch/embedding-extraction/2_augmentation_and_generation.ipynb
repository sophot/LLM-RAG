{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "059b4251",
   "metadata": {},
   "source": [
    "This notebook demonstrates RAG's \"Augmentation\" & \"Generation\" processes.\n",
    "\n",
    "> **_\"Augmentation\"_** is the process of combining information retrieved from external sources with the user input query and fed into the language model (Qwen, Llamma) to improve the quality and relevance of the generated answer.\n",
    ">\n",
    "> _Why is Augmentation Useful?_ </br>\n",
    "> Without augmentation, the LLM is guessing based only on its internal training. With augmentation:\n",
    ">\n",
    "> - You can provide **up-to-date** or **domain-specific** info\n",
    ">\n",
    "> - You reduce **hallucinations** (made-up facts)\n",
    ">\n",
    "> - You make smaller or fine-tuned models perform much better\n",
    ">\n",
    "> \n",
    "> </br> **_\"Generation\"_** is the final step where a **language model (LLM)** takes the **augmented input**—which includes the **original user query + retrieved documents**—and produces a natural language response.\n",
    "> \n",
    "> _Think of it like?_ </br>\n",
    "> - The LLM is a smart student.\n",
    "> - Retrieval gives it the right textbook pages. \n",
    "> - Augmentation is handing the student those pages along with the exam question—so it can give a smarter, informed answer.\n",
    "> - Generation is **informed by real data**, not just pretraining. The language model uses the retrieved context to generate **fact-based, grounded answers**, reducing hallucination and improving domain-specific performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6c242d",
   "metadata": {},
   "source": [
    "## **Create the Vector Store**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "977da33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Set up Chromadb\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from chromadb.api.models import Collection\n",
    "\n",
    "def create_vector_store(db_path: str, model_name: str) -> Collection:\n",
    "    \"\"\"\n",
    "    Creates a persistent ChromaDB vector store with OpenAI embeddings.\n",
    "\n",
    "    Args:\n",
    "        db_path (str): Path where the ChromaDB database will be stored.\n",
    "\n",
    "    Returns:\n",
    "        Collection: A ChromaDB collection object for storing and retrieving embedded vectors.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize a ChromaDB PersistentClient with the specified database path\n",
    "    client = chromadb.PersistentClient(path=db_path)\n",
    "    \n",
    "    # Create an embedding function using OpenAI's text embedding model\n",
    "    embeddings = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "        model_name=model_name,\n",
    "        device=device,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    # Create a new collection in the ChromaDB database with the embedding function\n",
    "    try:\n",
    "        db = client.create_collection(\n",
    "            name=\"pdf_chunks\",  # Name of the collection where embeddings will be stored\n",
    "            embedding_function=embeddings\n",
    "        )\n",
    "    except Exception as err:\n",
    "        db = client.get_collection(\n",
    "            name=\"pdf_chunks\",\n",
    "            embedding_function=embeddings\n",
    "        )\n",
    "\n",
    "    # Return the created ChromaDB collection\n",
    "    return db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "381183f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at Alibaba-NLP/gte-multilingual-base were not used when initializing NewModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "db_alibaba_gte = create_vector_store(db_path=\"./chroma_alibaba_gte.db\", model_name=\"Alibaba-NLP/gte-multilingual-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b916a609",
   "metadata": {},
   "source": [
    "## **Retrieve Chunks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45a30261",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List\n",
    "\n",
    "def retrieve_chunks(db: Collection, query: str, n_results: int = 2) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Retrieves relevant chunks from the  vector store for the given query.\n",
    "\n",
    "    Args:\n",
    "        db (Collection): The vector store object\n",
    "        query (str): The search query text.\n",
    "        n_results (int, optional): The number of relevant chunks to retrieve. Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        List[Any]: A list of relevant chunks retrieved from the vector store.\n",
    "    \"\"\"\n",
    "\n",
    "    # Perform a query on the database to get the most relevant chunks\n",
    "    relevant_chunks = db.query(query_texts=[query], n_results=n_results)\n",
    "\n",
    "    # Return the retrieved relevant chunks\n",
    "    return relevant_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1c48940",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the attention mechanism?\"\n",
    "relevant_chunks = retrieve_chunks(db=db_alibaba_gte, query=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c29e07",
   "metadata": {},
   "source": [
    "## **Build Context**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d6be759",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_context(relevant_chunks) -> str:\n",
    "    \"\"\"\n",
    "    Builds a single context string by combining texts from relevant chunks.\n",
    "\n",
    "    Args:\n",
    "        relevant_chunks: relevant chunks retrieved from the vector store.\n",
    "\n",
    "    Returns:\n",
    "        str: A single string containing all document chunks combined with newline separators.\n",
    "    \"\"\"\n",
    "\n",
    "    # combine the text from relevant chunks with newline separator\n",
    "    context = \"\\n\".join(relevant_chunks['documents'][0])\n",
    "\n",
    "    # Return the combined context string\n",
    "    return context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93fc73e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = build_context(relevant_chunks=relevant_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac80d98",
   "metadata": {},
   "source": [
    "## **Augment Prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05f8d319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_prompt(context, query):\n",
    "    \"\"\"\n",
    "    Generates a rag prompt based on the given context and query.\n",
    "\n",
    "    Args:\n",
    "        context (str): The context the LLM should use to answer the question.\n",
    "        query (str): The user query that needs to be answered based on the context.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated rag prompt.\n",
    "    \"\"\"\n",
    "\n",
    "    # Format the prompt with the provided context and query\n",
    "    rag_prompt = f\"\"\" You are an AI model trained for question answering. You should answer the given question based on the given context only.\n",
    "    Question : {query}\n",
    "    \\n\n",
    "    Context : {context}\n",
    "    \\n\n",
    "    If the answer is not present in the given context, respond as: The answer to this question is not available in the provided content.\n",
    "    \"\"\"\n",
    "\n",
    "    # Return the formatted prompt\n",
    "    return rag_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "862e97bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " You are an AI model trained for question answering. You should answer the given question based on the given context only.\n",
      "    Question : What is the attention mechanism?\n",
      "    \n",
      "\n",
      "    Context :  Attention Is All You Need Ashish Vaswani\u0003 Google Brain avaswani@google.comNoam Shazeer\u0003 Google Brain noam@google.comNiki Parmar\u0003 Google Research nikip@google.comJakob Uszkoreit\u0003 Google Research usz@google.com Llion Jones\u0003 Google Research llion@google.comAidan N. Gomez\u0003y University of Toronto aidan@cs.toronto.eduŁukasz Kaiser\u0003 Google Brain lukaszkaiser@google.com Illia Polosukhin\u0003z illia.polosukhin@gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train.\n",
      "3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. 3.2.1 Scaled Dot-Product Attention We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the 3 Scaled Dot-Product Attention  Multi-Head Attention Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel. query with all keys, divide each bypdk, and apply a softmax function to obtain the weights on the values. In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q.\n",
      "    \n",
      "\n",
      "    If the answer is not present in the given context, respond as: The answer to this question is not available in the provided content.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "augment_prompt = augment_prompt(context=context, query=query)\n",
    "print(augment_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a73bddf",
   "metadata": {},
   "source": [
    "## **LLM Response**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf2ab40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def ask_llm(prompt):\n",
    "    \"\"\"\n",
    "    Sends a prompt to the Alibaba's Qwen LLM and returns the answer.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The augmented prompt.\n",
    "\n",
    "    Returns:\n",
    "        str: The LLM generated answer.\n",
    "    \"\"\"\n",
    "    \n",
    "    model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=\"auto\",\n",
    "    )\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = model.to(device)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    messages = [\n",
    "        # {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a344603",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "response = ask_llm(prompt=augment_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c74cdada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Query:\n",
      "'What is the attention mechanism?'\n",
      "\n",
      "LLM's Response:\n",
      "'The attention mechanism is a technique used in machine learning, particularly in natural language processing and computer vision tasks. It allows a model to focus on different parts of its input during training and inference.\n",
      "\n",
      "In the provided text, the attention mechanism is introduced as part of the Transformer architecture proposed by Ashish Vaswani et al., which aims to improve upon traditional recurrent or convolutional neural network-based sequence transduction models. The attention mechanism in Transformers uses self-attention to weigh the importance of different parts of the input sequence during the computation of a contextual representation. This helps the model to capture dependencies between elements within sequences more effectively than previous architectures did.'\n"
     ]
    }
   ],
   "source": [
    "print(f\"User Query:\\n'{query}'\\n\\nLLM's Response:\\n'{response}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0baff0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
