{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates RAG's \"Indexing\" & \"Retrieval\" processes.\n",
    "\n",
    "> **_\"Indexing\"_** processes raw documents (pdf file in this example) by extracting their content (parsing) then splitting them into smaller, meaningful chunks. These chunks are then converted into vector embeddings using an embedding model (two embedding models is demonstrated here :D) and stored in a vector database (ChromaDB) for efiicient retrieval during query-time.\n",
    ">\n",
    "> Steps in Indexing:\n",
    "> - _Parsing_ \n",
    ">   - Extract raw text from documents (e.g., PDFs, Web pages, etc)\n",
    "> - _Chunking_ \n",
    ">   - Split large text into smaller chunks (e.g., paragraphs or sentences) to improve retrieval granularity.\n",
    "> - _Embedding_ \n",
    ">   - Each chunk is passed through an embedding model (like a Sentence Transformer or BGE) to convert it into a dense vector representation.\n",
    "> - _Storing in a Vector Database / Index_ \n",
    ">   - These embeddings are stored in a vector index using tools like ChromaDb, FAISS, Pinecone.\n",
    ">\n",
    "> **_\"Retrieval\"_** is the process of **finding and returning the most relevant documents or text chunks** from an external knowledge base (which was previously indexed) in response to a user query or prompt.\n",
    "> \n",
    "> Steps in Retrieval:\n",
    "> -  _User Input / Query_ \n",
    ">    - Example: ```What is the attention mechanism?```\n",
    "> - _Query Embedding_ \n",
    ">   - The input is passed through an **embedding model** to get a **dense vector representation**.\n",
    "> - _Similarity Search_ \n",
    ">   - This vector is compared with all the document vectors in the **vector index** (created during the indexing step) using **cosine similarity**, **dot product**, or **L2 distance**. \n",
    "> - _Top-K Retrieval_\n",
    ">   - The top ```K``` most similar document chunks are retrieved. These are the ones the model considers **most relevant to the input query**.\n",
    "> - _Return Results_\n",
    ">   - The retrieved chunks (usually 3–10, depending on your setup) are passed into the generation model as part of the prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Extract Text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "def text_extract(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts text from all pages of a given PDF file.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted text from the PDF, concatenated with newline separators.\n",
    "    \"\"\"\n",
    "\n",
    "    # An empty list to store extracted text from PDF pages\n",
    "    pdf_pages = []\n",
    "\n",
    "    # Open the PDF file in binary read mode\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "\n",
    "        # Create a PdfReader object to read the PDF\n",
    "        pdf_reader = PdfReader(file)\n",
    "\n",
    "        # Iterate through all pages in the PDF\n",
    "        for page in pdf_reader.pages:\n",
    "\n",
    "            # Extract text from the current page\n",
    "            text = page.extract_text()\n",
    "\n",
    "            # Append the extracted text to the list\n",
    "            pdf_pages.append(text)\n",
    "\n",
    "    # Join all extracted text using newline separator\n",
    "    pdf_text = \"\\n\".join(pdf_pages)\n",
    "\n",
    "    # Return the extracted text as a single string\n",
    "    return pdf_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the PDF file\n",
    "import requests\n",
    "\n",
    "pdf_url = 'https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf'\n",
    "response = requests.get(pdf_url)\n",
    "\n",
    "pdf_path = 'attention_is_all_you_need.pdf'\n",
    "with open(pdf_path, 'wb') as file:\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_text = text_extract(pdf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Chunk Text**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import re\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "def text_chunk(text: str, max_length: int = 1000) -> List[str]:\n",
    "    \"\"\"\n",
    "    Splits a given text into chunks while ensuring that sentences remain intact.\n",
    "\n",
    "    The function maintains sentence boundaries by splitting based on punctuation\n",
    "    (. ! ?) and attempts to fit as many sentences as possible within `max_length`\n",
    "    per chunk.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to be chunked.\n",
    "        max_length (int, optional): Maximum length of each chunk. Default is 1000.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of text chunks, each containing full sentences.\n",
    "    \"\"\"\n",
    "\n",
    "    # Split text into sentences while ensuring punctuation (. ! ?) stays at the end\n",
    "    sentences = deque(re.split(r'(?<=[.!?])\\s+', text.replace('\\n', ' ')))\n",
    "\n",
    "    # An empty list to store the final chunks\n",
    "    chunks = []\n",
    "\n",
    "    # Temporary string to hold the current chunk\n",
    "    chunk_text = \"\"\n",
    "\n",
    "    while sentences:\n",
    "        # Access sentence from the deque and strip any extra spaces\n",
    "        sentence = sentences.popleft().strip()\n",
    "\n",
    "        # Check if the sentence is non-empty before processing\n",
    "        if sentence:\n",
    "            # If adding this sentence exceeds max_length and chunk_text is not empty, store the current chunk\n",
    "            if len(chunk_text) + len(sentence) > max_length and chunk_text:\n",
    "\n",
    "                # Save the current chunk\n",
    "                chunks.append(chunk_text)\n",
    "\n",
    "                # Start a new chunk with the current sentence\n",
    "                chunk_text = sentence\n",
    "            else:\n",
    "                # Append the sentence to the current chunk with a space\n",
    "                chunk_text += \" \" + sentence\n",
    "\n",
    "    # Add the last chunk if there's any remaining text\n",
    "    if chunk_text:\n",
    "        chunks.append(chunk_text)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = text_chunk(pdf_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Create the Vector Store**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Set up Chromadb\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from chromadb.api.models import Collection\n",
    "\n",
    "def create_vector_store(db_path: str, model_name: str) -> Collection:\n",
    "    \"\"\"\n",
    "    Creates a persistent ChromaDB vector store with OpenAI embeddings.\n",
    "\n",
    "    Args:\n",
    "        db_path (str): Path where the ChromaDB database will be stored.\n",
    "\n",
    "    Returns:\n",
    "        Collection: A ChromaDB collection object for storing and retrieving embedded vectors.\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Initialize a ChromaDB PersistentClient with the specified database path\n",
    "    client = chromadb.PersistentClient(path=db_path)\n",
    "    \n",
    "    # Create an embedding function using OpenAI's text embedding model\n",
    "    embeddings = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "        model_name=model_name,\n",
    "        device=device,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    # Create a new collection in the ChromaDB database with the embedding function\n",
    "    try:\n",
    "        db = client.create_collection(\n",
    "            name=\"pdf_chunks\",  # Name of the collection where embeddings will be stored\n",
    "            embedding_function=embeddings\n",
    "        )\n",
    "    except Exception as err:\n",
    "        db = client.get_collection(\n",
    "            name=\"pdf_chunks\",\n",
    "            embedding_function=embeddings\n",
    "        )\n",
    "\n",
    "    # Return the created ChromaDB collection\n",
    "    return db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Alibaba-NLP/gte-multilingual-base were not used when initializing NewModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "db_default = create_vector_store(db_path=\"./chroma_defautl.db\", model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "db_alibaba_gte = create_vector_store(db_path=\"./chroma_alibaba_gte.db\", model_name=\"Alibaba-NLP/gte-multilingual-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert chunks into vector store\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "def insert_chunks_vectordb(chunks: List[str], db: Collection, file_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Inserts text chunks into a ChromaDB vector store with metadata.\n",
    "\n",
    "    Args:\n",
    "        chunks (List[str]): List of text chunks to be stored.\n",
    "        db (Collection): The ChromaDB collection where the chunks will be inserted.\n",
    "        file_path (str): Path of the source file for metadata.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract the file name from the given file path\n",
    "    file_name = os.path.basename(file_path)\n",
    "\n",
    "    # Generate unique IDs for each chunk\n",
    "    id_list = [str(uuid.uuid4()) for _ in range(len(chunks))]\n",
    "\n",
    "    # Create metadata for each chunk, storing the chunk index and source file name\n",
    "    metadata_list = [{\"chunk\": i, \"source\": file_name} for i in range(len(chunks))]\n",
    "\n",
    "    # Define batch size for inserting chunks to optimize performance\n",
    "    batch_size = 40\n",
    "\n",
    "    # Insert chunks into the database in batches\n",
    "    for i in range(0, len(chunks), batch_size):\n",
    "        end_id = min(i + batch_size, len(chunks))  # Ensure we don't exceed list length\n",
    "\n",
    "        # Add the batch of chunks to the vector store\n",
    "        db.add(\n",
    "            documents=chunks[i:end_id],\n",
    "            metadatas=metadata_list[i:end_id],\n",
    "            ids=id_list[i:end_id]\n",
    "        )\n",
    "\n",
    "    print(f\"{len(chunks)} chunks added to the vector store\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 chunks added to the vector store\n",
      "36 chunks added to the vector store\n"
     ]
    }
   ],
   "source": [
    "insert_chunks_vectordb(chunks=chunks, db=db_default, file_path=pdf_path)\n",
    "insert_chunks_vectordb(chunks=chunks, db=db_alibaba_gte, file_path=pdf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Retrieve Chunks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List\n",
    "\n",
    "def retrieve_chunks(db: Collection, query: str, n_results: int = 2) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Retrieves relevant chunks from the  vector store for the given query.\n",
    "\n",
    "    Args:\n",
    "        db (Collection): The vector store object\n",
    "        query (str): The search query text.\n",
    "        n_results (int, optional): The number of relevant chunks to retrieve. Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        List[Any]: A list of relevant chunks retrieved from the vector store.\n",
    "    \"\"\"\n",
    "\n",
    "    # Perform a query on the database to get the most relevant chunks\n",
    "    relevant_chunks = db.query(query_texts=[query], n_results=n_results)\n",
    "\n",
    "    # Return the retrieved relevant chunks\n",
    "    return relevant_chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Retrieve with ChromaDB's default Embedding Model \n",
    "Model : \"sentence-transformers/all-MiniLM-L6-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference 0\n",
      "'We also experimented with using learned positional embeddings [ 8] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training. 4 Why Self-Attention In this section we compare various aspects of self-attention layers to the recurrent and convolu- tional layers commonly used for mapping one variable-length sequence of symbol representations (x1;:::;x n)to another sequence of equal length (z1;:::;z n), withxi;zi2Rd, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata. One is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.'\n",
      "\n",
      "Reference 1\n",
      "'In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2. Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 22, 23, 19].'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the attention mechanism?\"\n",
    "relevant_chunks = retrieve_chunks(db=db_default, query=query)\n",
    "\n",
    "for i, doc in enumerate(relevant_chunks[\"documents\"][0]):\n",
    "    print(f\"Reference {i}\\n'{doc}'\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Retrieve with Alibaba's General Text Embedding model\n",
    "Model : \"Alibaba-NLP/gte-multilingual-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference 0\n",
      "' Attention Is All You Need Ashish Vaswani\u0003 Google Brain avaswani@google.comNoam Shazeer\u0003 Google Brain noam@google.comNiki Parmar\u0003 Google Research nikip@google.comJakob Uszkoreit\u0003 Google Research usz@google.com Llion Jones\u0003 Google Research llion@google.comAidan N. Gomez\u0003y University of Toronto aidan@cs.toronto.eduŁukasz Kaiser\u0003 Google Brain lukaszkaiser@google.com Illia Polosukhin\u0003z illia.polosukhin@gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train.'\n",
      "\n",
      "Reference 1\n",
      "'3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. 3.2.1 Scaled Dot-Product Attention We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the 3 Scaled Dot-Product Attention  Multi-Head Attention Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel. query with all keys, divide each bypdk, and apply a softmax function to obtain the weights on the values. In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q.'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the attention mechanism?\"\n",
    "relevant_chunks = retrieve_chunks(db=db_alibaba_gte, query=query)\n",
    "\n",
    "for i, doc in enumerate(relevant_chunks[\"documents\"][0]):\n",
    "    print(f\"Reference {i}\\n'{doc}'\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "I'd say that Alibaba's Embeddisg model does better at retrieving relevant piece of information, thank to its superior ability to capture semantic meaning of texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
