{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBldBXEn-s2G"
      },
      "source": [
        "# **RAG from Scratch**\n",
        "\n",
        "Authored by [Kalyan KS](https://www.linkedin.com/in/kalyanksnlp/). To stay updated with LLM, RAG and Agent updates, you can follow me on [Twitter](https://x.com/kalyan_kpl).\n",
        "\n",
        "- Step-1 : Extract text\n",
        "- Step-2 : Chunk the extracted text\n",
        "- Step-3 : Create a vector store with the chunks\n",
        "- Step-4 : Create a retriever which returns the relevant chunks\n",
        "- Step-5 : Build context from the relevant chunk texts\n",
        "- Step-6 : Build the RAG pipeline\n",
        "- Step-7 : Run the RAG pipeline to get the answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZExw2kIQ-vIu"
      },
      "source": [
        "## **Install libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VeQwqlMC9mOE",
        "outputId": "38534f94-8d47-4f44-8f90-6e60077cf0b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/dill-0.3.9-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/nvfuser-0.2.13a0+0d33366-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/opt_einsum-3.4.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/texttable-1.7.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_utilities-0.11.8-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/looseversion-1.3.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/igraph-0.11.8-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install -qU PyPDF2 chromadb litellm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BItWSfV5_YEP"
      },
      "source": [
        "## **Extract Text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wl1GlnRz_kv3"
      },
      "outputs": [],
      "source": [
        "from PyPDF2 import PdfReader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "j9ud4GFX_fMz"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from PyPDF2 import PdfReader\n",
        "\n",
        "def text_extract(pdf_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Extracts text from all pages of a given PDF file.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): Path to the PDF file.\n",
        "\n",
        "    Returns:\n",
        "        str: Extracted text from the PDF, concatenated with newline separators.\n",
        "    \"\"\"\n",
        "\n",
        "    # An empty list to store extracted text from PDF pages\n",
        "    pdf_pages = []\n",
        "\n",
        "    # Open the PDF file in binary read mode\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "\n",
        "        # Create a PdfReader object to read the PDF\n",
        "        pdf_reader = PdfReader(file)\n",
        "\n",
        "        # Iterate through all pages in the PDF\n",
        "        for page in pdf_reader.pages:\n",
        "\n",
        "            # Extract text from the current page\n",
        "            text = page.extract_text()\n",
        "\n",
        "            # Append the extracted text to the list\n",
        "            pdf_pages.append(text)\n",
        "\n",
        "    # Join all extracted text using newline separator\n",
        "    pdf_text = \"\\n\".join(pdf_pages)\n",
        "\n",
        "    # Return the extracted text as a single string\n",
        "    return pdf_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ttlt3RYWAh8n"
      },
      "outputs": [],
      "source": [
        "# Download the PDF file\n",
        "import requests\n",
        "\n",
        "pdf_url = 'https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf'\n",
        "response = requests.get(pdf_url)\n",
        "\n",
        "pdf_path = 'attention_is_all_you_need.pdf'\n",
        "with open(pdf_path, 'wb') as file:\n",
        "    file.write(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_NK7uS8aAwxC"
      },
      "outputs": [],
      "source": [
        "pdf_text = text_extract(pdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIuE4C3WBCw7",
        "outputId": "3168bc37-5076-4735-cf7e-4e257f465339"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attention Is All You Need\n",
            "Ashish Vaswani\u0003\n",
            "Google Brain\n",
            "avaswani@google.comNoam Shazeer\u0003\n",
            "Google Brain\n",
            "noam@google.comNiki Parmar\u0003\n",
            "Google Research\n",
            "nikip@google.comJakob Uszkoreit\u0003\n",
            "Google Research\n",
            "usz@google.com\n",
            "Llion Jones\u0003\n",
            "Google Research\n",
            "llion@google.comAidan N. Gomez\u0003y\n",
            "University of Toronto\n",
            "aidan@c\n"
          ]
        }
      ],
      "source": [
        "print(pdf_text[:300])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gFK0HfYBb5i"
      },
      "source": [
        "## **Chunk Text**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wsYaJ2xAQ-9D"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "import re\n",
        "from collections import deque\n",
        "\n",
        "\n",
        "def text_chunk(text: str, max_length: int = 1000) -> List[str]:\n",
        "    \"\"\"\n",
        "    Splits a given text into chunks while ensuring that sentences remain intact.\n",
        "\n",
        "    The function maintains sentence boundaries by splitting based on punctuation\n",
        "    (. ! ?) and attempts to fit as many sentences as possible within `max_length`\n",
        "    per chunk.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text to be chunked.\n",
        "        max_length (int, optional): Maximum length of each chunk. Default is 1000.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of text chunks, each containing full sentences.\n",
        "    \"\"\"\n",
        "\n",
        "    # Split text into sentences while ensuring punctuation (. ! ?) stays at the end\n",
        "    sentences = deque(re.split(r'(?<=[.!?])\\s+', text.replace('\\n', ' ')))\n",
        "\n",
        "    # An empty list to store the final chunks\n",
        "    chunks = []\n",
        "\n",
        "    # Temporary string to hold the current chunk\n",
        "    chunk_text = \"\"\n",
        "\n",
        "    while sentences:\n",
        "        # Access sentence from the deque and strip any extra spaces\n",
        "        sentence = sentences.popleft().strip()\n",
        "\n",
        "        # Check if the sentence is non-empty before processing\n",
        "        if sentence:\n",
        "            # If adding this sentence exceeds max_length and chunk_text is not empty, store the current chunk\n",
        "            if len(chunk_text) + len(sentence) > max_length and chunk_text:\n",
        "\n",
        "                # Save the current chunk\n",
        "                chunks.append(chunk_text)\n",
        "\n",
        "                # Start a new chunk with the current sentence\n",
        "                chunk_text = sentence\n",
        "            else:\n",
        "                # Append the sentence to the current chunk with a space\n",
        "                chunk_text += \" \" + sentence\n",
        "\n",
        "    # Add the last chunk if there's any remaining text\n",
        "    if chunk_text:\n",
        "        chunks.append(chunk_text)\n",
        "\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "stO6tscmToxv"
      },
      "outputs": [],
      "source": [
        "chunks = text_chunk(pdf_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrPVFAyXTsaQ",
        "outputId": "d71e16f7-b72b-424a-8aec-b2bbfff4c382"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of chunks = 36\n",
            " Attention Is All You Need Ashish Vaswani\u0003 Google Brain avaswani@google.comNoam Shazeer\u0003 Google Brain noam@google.comNiki Parmar\u0003 Google Research nikip@google.comJakob Uszkoreit\u0003 Google Research usz@google.com Llion Jones\u0003 Google Research llion@google.comAidan N. Gomez\u0003y University of Toronto aidan@cs.toronto.eduŁukasz Kaiser\u0003 Google Brain lukaszkaiser@google.com Illia Polosukhin\u0003z illia.polosukhin@gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train.\n"
          ]
        }
      ],
      "source": [
        "print(f\"Number of chunks = {len(chunks)}\")\n",
        "print(chunks[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyfi5blWT-cn"
      },
      "source": [
        "## **Create the Vector Store**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "G36-SWrFUANV"
      },
      "outputs": [],
      "source": [
        "# Set up Chromadb\n",
        "import chromadb\n",
        "from chromadb.utils import embedding_functions\n",
        "from chromadb.api.models import Collection\n",
        "\n",
        "def create_vector_store(db_path: str) -> Collection:\n",
        "    \"\"\"\n",
        "    Creates a persistent ChromaDB vector store with OpenAI embeddings.\n",
        "\n",
        "    Args:\n",
        "        db_path (str): Path where the ChromaDB database will be stored.\n",
        "\n",
        "    Returns:\n",
        "        Collection: A ChromaDB collection object for storing and retrieving embedded vectors.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize a ChromaDB PersistentClient with the specified database path\n",
        "    client = chromadb.PersistentClient(path=db_path)\n",
        "\n",
        "    # Create a new collection in the ChromaDB database with the embedding function\n",
        "    db = client.create_collection(\n",
        "        name=\"pdf_chunks\",  # Name of the collection where embeddings will be stored\n",
        "    )\n",
        "\n",
        "    # Return the created ChromaDB collection\n",
        "    return db\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "db = create_vector_store(db_path=\"./chroma.db\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<chromadb.utils.embedding_functions.DefaultEmbeddingFunction at 0x7f5933ed9e50>"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "db._embedding_function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Mz5u2hy-UeUm"
      },
      "outputs": [],
      "source": [
        "# Insert chunks into vector store\n",
        "import os\n",
        "import uuid\n",
        "\n",
        "def insert_chunks_vectordb(chunks: List[str], db: Collection, file_path: str) -> None:\n",
        "    \"\"\"\n",
        "    Inserts text chunks into a ChromaDB vector store with metadata.\n",
        "\n",
        "    Args:\n",
        "        chunks (List[str]): List of text chunks to be stored.\n",
        "        db (Collection): The ChromaDB collection where the chunks will be inserted.\n",
        "        file_path (str): Path of the source file for metadata.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract the file name from the given file path\n",
        "    file_name = os.path.basename(file_path)\n",
        "\n",
        "    # Generate unique IDs for each chunk\n",
        "    id_list = [str(uuid.uuid4()) for _ in range(len(chunks))]\n",
        "\n",
        "    # Create metadata for each chunk, storing the chunk index and source file name\n",
        "    metadata_list = [{\"chunk\": i, \"source\": file_name} for i in range(len(chunks))]\n",
        "\n",
        "    # Define batch size for inserting chunks to optimize performance\n",
        "    batch_size = 40\n",
        "\n",
        "    # Insert chunks into the database in batches\n",
        "    for i in range(0, len(chunks), batch_size):\n",
        "        end_id = min(i + batch_size, len(chunks))  # Ensure we don't exceed list length\n",
        "\n",
        "        # Add the batch of chunks to the vector store\n",
        "        db.add(\n",
        "            documents=chunks[i:end_id],\n",
        "            metadatas=metadata_list[i:end_id],\n",
        "            ids=id_list[i:end_id]\n",
        "        )\n",
        "\n",
        "    print(f\"{len(chunks)} chunks added to the vector store\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/.cache/chroma/onnx_models/all-MiniLM-L6-v2/onnx.tar.gz: 100%|██████████| 79.3M/79.3M [00:06<00:00, 12.5MiB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "36 chunks added to the vector store\n"
          ]
        }
      ],
      "source": [
        "insert_chunks_vectordb(chunks=chunks, db=db, file_path=pdf_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duNeKQucZBnW"
      },
      "source": [
        "## **Retrieve Chunks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "OiLTV_9jZGCy"
      },
      "outputs": [],
      "source": [
        "from typing import Any, List\n",
        "\n",
        "def retrieve_chunks(db: Collection, query: str, n_results: int = 2) -> List[Any]:\n",
        "    \"\"\"\n",
        "    Retrieves relevant chunks from the  vector store for the given query.\n",
        "\n",
        "    Args:\n",
        "        db (Collection): The vector store object\n",
        "        query (str): The search query text.\n",
        "        n_results (int, optional): The number of relevant chunks to retrieve. Defaults to 2.\n",
        "\n",
        "    Returns:\n",
        "        List[Any]: A list of relevant chunks retrieved from the vector store.\n",
        "    \"\"\"\n",
        "\n",
        "    # Perform a query on the database to get the most relevant chunks\n",
        "    relevant_chunks = db.query(query_texts=[query], n_results=n_results)\n",
        "\n",
        "    # Return the retrieved relevant chunks\n",
        "    return relevant_chunks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"What is the attention mechanism?\"\n",
        "relevant_chunks = retrieve_chunks(db=db, query=query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['ids', 'embeddings', 'documents', 'uris', 'included', 'data', 'metadatas', 'distances'])"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "relevant_chunks.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ids': [['3a475760-7e99-4de2-9b08-ef362e6bc8da',\n",
              "   '132121d1-88d9-4710-8588-512fdc280a9d']],\n",
              " 'embeddings': None,\n",
              " 'documents': [['We also experimented with using learned positional embeddings [ 8] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training. 4 Why Self-Attention In this section we compare various aspects of self-attention layers to the recurrent and convolu- tional layers commonly used for mapping one variable-length sequence of symbol representations (x1;:::;x n)to another sequence of equal length (z1;:::;z n), withxi;zi2Rd, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata. One is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.',\n",
              "   'In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2. Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 22, 23, 19].']],\n",
              " 'uris': None,\n",
              " 'included': ['metadatas', 'documents', 'distances'],\n",
              " 'data': None,\n",
              " 'metadatas': [[{'source': 'attention_is_all_you_need.pdf', 'chunk': 17},\n",
              "   {'chunk': 5, 'source': 'attention_is_all_you_need.pdf'}]],\n",
              " 'distances': [[1.0410888195037842, 1.0439645051956177]]}"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "relevant_chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['We also experimented with using learned positional embeddings [ 8] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training. 4 Why Self-Attention In this section we compare various aspects of self-attention layers to the recurrent and convolu- tional layers commonly used for mapping one variable-length sequence of symbol representations (x1;:::;x n)to another sequence of equal length (z1;:::;z n), withxi;zi2Rd, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata. One is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.',\n",
              " 'In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2. Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 22, 23, 19].']"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "relevant_chunks[\"documents\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2. Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 22, 23, 19].'"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chunks[5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arKLWO02ZrrF"
      },
      "source": [
        "## **Build Context**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Re7I3y4iZwuz"
      },
      "outputs": [],
      "source": [
        "\n",
        "def build_context(relevant_chunks) -> str:\n",
        "    \"\"\"\n",
        "    Builds a single context string by combining texts from relevant chunks.\n",
        "\n",
        "    Args:\n",
        "        relevant_chunks: relevant chunks retrieved from the vector store.\n",
        "\n",
        "    Returns:\n",
        "        str: A single string containing all document chunks combined with newline separators.\n",
        "    \"\"\"\n",
        "\n",
        "    # combine the text from relevant chunks with newline separator\n",
        "    context = \"\\n\".join(relevant_chunks['documents'][0])\n",
        "\n",
        "    # Return the combined context string\n",
        "    return context\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-X0x6gtbp2p"
      },
      "source": [
        "## **Build RAG Pipeline**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azZAY-BraBwf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import Tuple\n",
        "\n",
        "def get_context(pdf_path: str, query: str, db_path: str) -> Tuple[str, str]:\n",
        "    \"\"\"\n",
        "    Retrieves the relevant chunks from the vector store and then builds context from them.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): The file path to the PDF document.\n",
        "        query (str): The query string to search within the vector store.\n",
        "        db_path (str): The file path to the persistent vector store database.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[str, str]: A tuple containing the context related to the query and the original query string.\n",
        "    \"\"\"\n",
        "\n",
        "    # Check if the vector store already exists\n",
        "    if os.path.exists(db_path):\n",
        "        print(\"Loading existing vector store...\")\n",
        "\n",
        "        # Initialize the persistent client for the existing database\n",
        "        client = chromadb.PersistentClient(path=db_path)\n",
        "\n",
        "        # Get the collection of PDF chunks from the existing vector store\n",
        "        db = client.get_collection(name=\"pdf_chunks\")\n",
        "    else:\n",
        "        print(\"Creating new vector store...\")\n",
        "\n",
        "        # Extract text from the provided PDF\n",
        "        pdf_text = text_extract(pdf_path)\n",
        "\n",
        "        # Chunk the extracted text\n",
        "        chunks = text_chunk(pdf_text)\n",
        "\n",
        "        # Create a new vector store\n",
        "        db = create_vector_store(db_path)\n",
        "\n",
        "        # Insert the text chunks into the vector store\n",
        "        insert_chunks_vectordb(chunks, db, pdf_path)\n",
        "\n",
        "    # Retrieve the relevant chunks based on the query\n",
        "    relevant_chunks = retrieve_chunks(db, query)\n",
        "\n",
        "    # Build the context from the relevant chunks\n",
        "    context = build_context(relevant_chunks)\n",
        "\n",
        "    # Return the context and the original query\n",
        "    return context, query\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzAC5TswbrnO"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_prompt(context: str, query: str) -> str:\n",
        "    \"\"\"\n",
        "    Generates a rag prompt based on the given context and query.\n",
        "\n",
        "    Args:\n",
        "        context (str): The context the LLM should use to answer the question.\n",
        "        query (str): The user query that needs to be answered based on the context.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated rag prompt.\n",
        "    \"\"\"\n",
        "\n",
        "    # Format the prompt with the provided context and query\n",
        "    rag_prompt = f\"\"\" You are an AI model trained for question answering. You should answer the\n",
        "    given question based on the given context only.\n",
        "    Question : {query}\n",
        "    \\n\n",
        "    Context : {context}\n",
        "    \\n\n",
        "    If the answer is not present in the given context, respond as: The answer to this question is not available\n",
        "    in the provided content.\n",
        "    \"\"\"\n",
        "\n",
        "    # Return the formatted prompt\n",
        "    return rag_prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LxkJY_gcd5g"
      },
      "outputs": [],
      "source": [
        "from litellm import completion\n",
        "\n",
        "def get_response(rag_prompt: str) -> str:\n",
        "    \"\"\"\n",
        "    Sends a prompt to the OpenAI LLM and returns the answer.\n",
        "\n",
        "    Args:\n",
        "        rag_prompt (str): The rag prompt.\n",
        "\n",
        "    Returns:\n",
        "        str: The LLM generated answer.\n",
        "    \"\"\"\n",
        "    # Specify the LLM to use\n",
        "    model = \"openai/gpt-4o-mini\"\n",
        "\n",
        "    # Prepare the message to be sent to the model\n",
        "    messages = [{\"role\": \"user\", \"content\": rag_prompt}]\n",
        "\n",
        "    # Call the completion function to get a response from the model\n",
        "    response = completion(model=model, messages=messages, temperature=0)\n",
        "\n",
        "    # Return the answer\n",
        "    answer = response.choices[0].message.content\n",
        "    return answer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQLzNtoWd3pd"
      },
      "outputs": [],
      "source": [
        "\n",
        "def rag_pipeline(pdf_path: str, query: str, db_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Runs a Retrieval-Augmented Generation (RAG) pipeline to retrieve context from a vector store,\n",
        "    generate the rag prompt, and then get the answer from the model.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): The file path to the PDF document from which context is extracted.\n",
        "        query (str): The query for which a response is needed, based on the context.\n",
        "        db_path (str): The file path to the persistent vector store database used for context retrieval.\n",
        "\n",
        "    Returns:\n",
        "        str: The model's response based on the context and the provided query.\n",
        "    \"\"\"\n",
        "\n",
        "    # get the context\n",
        "    context, query = get_context(pdf_path, query, db_path)\n",
        "\n",
        "    # Generate the rag prompt based on the context and query\n",
        "    rag_prompt = get_prompt(context, query)\n",
        "\n",
        "    # Get the response from the model using the rag prompt\n",
        "    response = get_response(rag_prompt)\n",
        "\n",
        "    # Return the model's response\n",
        "    return response\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdIxhjaBn6d4"
      },
      "source": [
        "## **Run RAG Pipeline**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyAPVswXegm8"
      },
      "outputs": [],
      "source": [
        "# Set the chroma DB path\n",
        "current_dir = \"/content/rag\"\n",
        "persistent_directory = os.path.join(current_dir, \"db\", \"chroma_db_pdf\")\n",
        "\n",
        "# PDF path\n",
        "pdf_path = \"attention_is_all_you_need.pdf\"\n",
        "\n",
        "# RAG query\n",
        "query = \"What is the transformer architecture?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZcKzayselVs",
        "outputId": "407c7a31-e8df-42f7-b853-16dde0ee2593"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating new vector store...\n",
            "36 chunks added to the vector store\n"
          ]
        }
      ],
      "source": [
        "# Run the RAG pipeline\n",
        "answer = rag_pipeline(pdf_path, query, persistent_directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djQeTMMNeoDB",
        "outputId": "1fa18a3f-8178-4b70-c407-9cc2f5f24e77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query:What is the transformer architecture?\n",
            "Generated answer:The Transformer architecture is a model that relies entirely on an attention mechanism to draw global dependencies between input and output, eschewing recurrence. It consists of stacked self-attention and point-wise, fully connected layers for both the encoder and decoder. The encoder is composed of a stack of six identical layers, each with a multi-head self-attention mechanism and a position-wise fully connected feed-forward network, along with residual connections and layer normalization. This architecture allows for significant parallelization and achieves state-of-the-art translation quality with relatively short training times.\n"
          ]
        }
      ],
      "source": [
        "print(f\"Query:{query}\")\n",
        "print(f\"Generated answer:{answer}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XJ4ulaInq48",
        "outputId": "81675e73-da48-4351-8832-0ebc77400584"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading existing vector store...\n",
            "Query:What is self-attention?\n",
            "Generated answer:Self-attention, sometimes called intra-attention, is an attention mechanism that relates different positions of a single sequence in order to compute a representation of that sequence. It has been successfully used in various tasks, including reading comprehension, abstractive summarization, textual entailment, and learning task-independent sentence representations.\n"
          ]
        }
      ],
      "source": [
        "# RAG query\n",
        "query = \"What is self-attention?\"\n",
        "\n",
        "# Run the RAG pipeline\n",
        "answer = rag_pipeline(pdf_path, query, persistent_directory)\n",
        "\n",
        "print(f\"Query:{query}\")\n",
        "print(f\"Generated answer:{answer}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
